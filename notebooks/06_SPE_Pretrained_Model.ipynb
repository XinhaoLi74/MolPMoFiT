{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from utils import *\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded folder (named \"models\") contains:\n",
    "1. vocabulary: ChEMBL_LM_SPE_vocab.pkl\n",
    "2. language model encoder (pretrained model weights): ChEMBL_spe_encoder.pth\n",
    "3. SPE tokens: SPE_ChEMBL.txt (generated from ChEMBL25, see more [here](https://github.com/XinhaoLi74/SmilesPE/blob/master/Examples/train_SPE.ipynb))\n",
    "\n",
    "**Note**:The `TextClasDataBunch` function needs a `path` as input (see more [here](https://fastai1.fast.ai/text.data.html#TextDataBunch.from_df)). The path should be the parent folder of the downloaded \"models\" folder. This is due to the fact that `learner.load_encoder()` will load the model from `path/models/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load the vocabulary\n",
    "2. initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model path\n",
    "path = Path('../results/SPE_Pretrained/') # the parent folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path}/models/ChEMBL_LM_SPE_vocab.pkl', 'rb') as f:\n",
    "    orig_itos = pickle.load(f)\n",
    "    \n",
    "vocab = Vocab(orig_itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [SmilesPE](https://github.com/XinhaoLi74/SmilesPE) package needs to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from SmilesPE.tokenizer import *\n",
    "\n",
    "spe_vob= codecs.open(f'{path}/models/SPE_ChEMBL.txt')\n",
    "spe = SPE_Tokenizer(spe_vob, exclusive_tokens=special_tokens)\n",
    "tok = Tokenizer(partial(MolTokenizer_SPE, spe), n_cpus=6, pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps should be the same as descripted in `05_Pretrained_Models.ipynb`. The encoder is named `ChEMBL_spe_encoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>p_np</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>[C@]14([C@H]([C@H]3[C@](F)([C@@H](O)C1)[C@@]2(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>C1=NC3=C([N]1CCNC(C(C2=CC=CC=C2)O)C)C(N(C)C(N3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>C1=C(C(=CC(=C1)Cl)Cl)OCCCN(CC#C)C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>C1=C(C(OCC)=O)[N](C=N1)C(C)C2=CC=C(C=C2)F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>CN(C1CCCC[C@H]1N2CCCC2)C(=O)Cc3ccc(Cl)c(Cl)c3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles  p_np\n",
       "1386  [C@]14([C@H]([C@H]3[C@](F)([C@@H](O)C1)[C@@]2(...     1\n",
       "1049  C1=NC3=C([N]1CCNC(C(C2=CC=CC=C2)O)C)C(N(C)C(N3...     1\n",
       "1101                  C1=C(C(=CC(=C1)Cl)Cl)OCCCN(CC#C)C     1\n",
       "1250          C1=C(C(OCC)=O)[N](C=N1)C(C)C2=CC=C(C=C2)F     1\n",
       "645       CN(C1CCCC[C@H]1N2CCCC2)C(=O)Cc3ccc(Cl)c(Cl)c3     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbbp = pd.read_csv('../data/QSAR/bbbp.csv').sample(n=500)\n",
    "print(bbbp.shape)\n",
    "bbbp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 50 45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(bbbp,\n",
    "    test_size=0.1, shuffle = True, random_state = 8)\n",
    "\n",
    "train, val = train_test_split(train,\n",
    "    test_size=0.1, shuffle = True, random_state = 42)\n",
    "\n",
    "print(train.shape[0], test.shape[0], val.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinhao/miniconda3/envs/fastaiv1/lib/python3.6/site-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinhao/miniconda3/envs/fastaiv1/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "bs = 128 #batch size\n",
    "\n",
    "qsar_db = TextClasDataBunch.from_df(path, train, val, bs=bs, tokenizer=tok, \n",
    "                                    chunksize=50000, text_cols='smiles',label_cols='p_np', \n",
    "                                    vocab=vocab, max_vocab=60000, include_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learner = text_classifier_learner(qsar_db, AWD_LSTM, pretrained=False, drop_mult=0.1, callback_fns=AUROC)\n",
    "cls_learner.load_encoder('ChEMBL_spe_encoder')\n",
    "cls_learner.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_learner.fit_one_cycle(4, 3e-2, moms=(0.8,0.7))\n",
    "cls_learner.freeze_to(-2)\n",
    "cls_learner.fit_one_cycle(4, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))\n",
    "cls_learner.freeze_to(-3)\n",
    "cls_learner.fit_one_cycle(4, slice(5e-4/(2.6**4),5e-4), moms=(0.8,0.7))\n",
    "cls_learner.unfreeze()\n",
    "cls_learner.fit_one_cycle(6, slice(5e-5/(2.6**4),5e-5), moms=(0.8,0.7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
